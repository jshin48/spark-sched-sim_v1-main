trainer:
  # where the training happens (cpu, cuda, cuda:0, ...)
  # note: rollouts are always collected using only the CPU
  device: 'cpu'

  # name of the trainer's class
  trainer_cls: 'PPO'

  # number of training iterations
  num_iterations: 100

  # number of unique job sequences sampled per training iteration
  num_sequences: 2

  # number of rollouts experienced per unique job sequence
  # `num_sequences` x `num_rollouts`
  #  = total number of rollouts per training iteration
  #  = number of rollout workers running in parallel
  num_rollouts: 2

  # base random seed; each worker gets its own seed which is offset from this.
  seed: 42

  # name of directory where all training artifacts are saved (e.g. tensorboard)
  artifacts_dir: 'artifacts'

  # if checkpointing_freq = n, then every n iterations, the best model from the
  # past m iterations is saved
  checkpointing_freq: 50

  # if true, then records training metrics to a tensorboard file
  use_tensorboard: False

  # PPO: number of times to train through all of the data from the most recent
  # iteration
  num_epochs: 3

  # PPO: number of batches to split the last iteration's training data into
  num_batches: 10

  # PPO: hyperparameter for clamping the importance sampling ratio
  clip_range: .2

  # PPO: end training cycle if approximate KL divergence exceeds `target_kl`
  target_kl: .01

  # PPO: coefficient of entropy bonus term (if 0 then no entropy bonus)
  entropy_coeff: .04

  # discount factor for (continuously) discounted returns
  beta_discount: 5.e-3

  # max reward window size for differential returns
  # reward_buff_cap: 200000

  # note: only one of `beta_discount` and `reward_buff_cap` must be specified,
  # indicating whether to use discounted or differential returns

  # optimizer settings
  opt_cls: 'Adam'
  opt_kwargs: 
    lr: 3.e-4
  max_grad_norm: .5


agent:
  agent_cls: 'HyperHeuristicScheduler'
  embed_dim: 16
  gnn_mlp_kwargs:
    hid_dims: [32, 16] #JS:42
    act_cls: 'LeakyReLU'
    act_kwargs:
      inplace: True
      negative_slope: .2
  policy_mlp_kwargs:
    hid_dims: [64, 64]
    act_cls: 'Tanh'
  num_heuristics: 3
  list_heuristics: [ 'MC','WSCPT' ]
  input_feature: [ 'num_queue',"glob" ]
  num_resource_heuristics: 2
  list_resource_heuristics: [ 'Maximum','DRA' ]
  # choose from 'Random', 'DNN', 'DRA', 'HyperHeuristic'
  resource_allocation: 'DNN'



env:
  num_executors: 10
  job_arrival_cap: 1000
  job_arrival_rate: 8.e-6
  moving_delay: 1000.
  warmup_delay: 1000.
  pod_creation_time: 100.
  data_sampler_cls: 'AlibabaDataSampler'
  mean_time_limit: 2.e+8
  # choose from None, 'static' (integer), 'DTS'
  splitting_rule: 'None' #'DTS'

